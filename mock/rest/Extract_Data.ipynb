{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://supermarket-scrape-website.vercel.app/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'{url}/api/category')\n",
    "temp_categories = json.loads(response.content)\n",
    "temp_categories = temp_categories['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confectionery', 'delicatessen', 'household', 'winecellar', 'fruitsandvegetables', 'organic', 'pets', 'chilledanddairy', 'fish', 'butcher']\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "for category in temp_categories:\n",
    "    categories.append(category['key'])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './json_dumps/'\n",
    "for category in categories:\n",
    "    if not os.path.exists(folder_path+category):\n",
    "        os.makedirs(folder_path+category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageContent(category, page):\n",
    "    res = requests.get(f'{url}/api/product?category={category}&page={page}')\n",
    "    if res.status_code==200:\n",
    "        return json.loads(res.content)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJsonToFile(category, page, file):\n",
    "    with open (f'./json_dumps/{category}/{page}.json', 'w') as json_file:\n",
    "        json.dump(file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping confectionery page 1\n",
      "Scraping confectionery page 2\n",
      "Scraping confectionery page 3\n",
      "Scraping confectionery page 4\n",
      "Scraping confectionery page 5\n",
      "Scraping delicatessen page 1\n",
      "Scraping delicatessen page 2\n",
      "Scraping delicatessen page 3\n",
      "Scraping delicatessen page 4\n",
      "Scraping delicatessen page 5\n",
      "Scraping household page 1\n",
      "Scraping household page 2\n",
      "Scraping household page 3\n",
      "Scraping household page 4\n",
      "Scraping household page 5\n",
      "Scraping winecellar page 1\n",
      "Scraping winecellar page 2\n",
      "Scraping winecellar page 3\n",
      "Scraping winecellar page 4\n",
      "Scraping winecellar page 5\n",
      "Scraping fruitsandvegetables page 1\n",
      "Scraping fruitsandvegetables page 2\n",
      "Scraping fruitsandvegetables page 3\n",
      "Scraping fruitsandvegetables page 4\n",
      "Scraping fruitsandvegetables page 5\n",
      "Scraping organic page 1\n",
      "Scraping organic page 2\n",
      "Scraping organic page 3\n",
      "Scraping organic page 4\n",
      "Scraping organic page 5\n",
      "Scraping pets page 1\n",
      "Scraping pets page 2\n",
      "Scraping pets page 3\n",
      "Scraping pets page 4\n",
      "Scraping pets page 5\n",
      "Scraping chilledanddairy page 1\n",
      "Scraping chilledanddairy page 2\n",
      "Scraping chilledanddairy page 3\n",
      "Scraping chilledanddairy page 4\n",
      "Scraping chilledanddairy page 5\n",
      "Scraping fish page 1\n",
      "Scraping fish page 2\n",
      "Scraping fish page 3\n",
      "Scraping fish page 4\n",
      "Scraping fish page 5\n",
      "Scraping butcher page 1\n",
      "Scraping butcher page 2\n",
      "Scraping butcher page 3\n",
      "Scraping butcher page 4\n",
      "Scraping butcher page 5\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    page = 1\n",
    "    pageEnd = False\n",
    "    while not pageEnd:\n",
    "        print (f'Scraping {category} page {page}')\n",
    "        content = getPageContent(category, page)\n",
    "        writeJsonToFile(category, page, content)\n",
    "        pageEnd = len(content['products']) == 0\n",
    "        page+=1\n",
    "        sleep_duration = random.uniform(1, 5)\n",
    "        time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = './json_dumps/'\n",
    "categories = os.listdir(source_folder)\n",
    "\n",
    "if not os.path.exists('json_combined'):\n",
    "    os.makedirs('json_combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining butcher\n",
      "Combining chilledanddairy\n",
      "Combining confectionery\n",
      "Combining delicatessen\n",
      "Combining fish\n",
      "Combining fruitsandvegetables\n",
      "Combining household\n",
      "Combining organic\n",
      "Combining pets\n",
      "Combining winecellar\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    print(f'Combining {category}')\n",
    "    all_product_details = []\n",
    "    files = os.listdir(f'{source_folder}{category}')\n",
    "    for file in files:\n",
    "        with open(f'{source_folder}{category}/{file}', 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for item in data['products']:\n",
    "                all_product_details.append(item)\n",
    "    df = pd.DataFrame(all_product_details)\n",
    "    df.to_csv(f'./json_combined/{category}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = './json_combined/'\n",
    "files = os.listdir(source_folder)\n",
    "combined_df = pd.concat([pd.read_csv(os.path.join(source_folder, file)) for file in files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('json_final'):\n",
    "    os.makedirs('json_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('./json_final/rest_products.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supermarket-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
